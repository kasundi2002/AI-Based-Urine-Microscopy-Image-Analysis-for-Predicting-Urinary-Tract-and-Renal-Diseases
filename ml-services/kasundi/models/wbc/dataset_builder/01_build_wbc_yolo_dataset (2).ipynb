{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcabu6XA7YC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfabc8c2-7741-4a43-d57a-9a7d45c5af91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip UMID.zip and Urinary_Sediment.zip into data/raw/...\n",
        "%cd /content/drive/MyDrive/urine_uti/ml-services/member1/data/raw\n",
        "!unzip -q UMID.zip -d UMID\n",
        "!unzip -q Urinary_Sediment.zip -d Urinary_Sediment"
      ],
      "metadata": {
        "id": "kpiXG7fjAxUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Make a tiny “label normalizer” - This function handles case, underscores, plurals"
      ],
      "metadata": {
        "id": "DybxL3RvCYCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# canonical classes for this specialist\n",
        "CANONICAL = {\"WBC\": 0}   # single-class detector; id 0\n",
        "\n",
        "# all names that should map to WBC\n",
        "WBC_ALIASES = {\n",
        "    \"wbc\", \"wbcs\", \"white blood cell\", \"white blood cells\",\n",
        "    \"leukocyte\", \"leukocytes\", \"pus cell\", \"pus cells\", \"pus_cell\", \"pus_cells\"\n",
        "}\n",
        "\n",
        "def normalize_label(name: str):\n",
        "    \"\"\"Return canonical class name or None if we should ignore.\"\"\"\n",
        "    if name is None:\n",
        "        return None\n",
        "    # tidy up\n",
        "    s = name.strip().lower()\n",
        "    s = s.replace(\"-\", \" \").replace(\"_\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s)  # collapse spaces\n",
        "\n",
        "    if s in WBC_ALIASES:\n",
        "        return \"WBC\"\n",
        "    # ignore everything else for this specialist (RBC, epithelial, crystals, etc.)\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "IqZ1Z6OACaNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2.UMID → YOLO (WBC only) with normalization - filter to WBC-ish rows and write YOLO txt files."
      ],
      "metadata": {
        "id": "EPCcW4J5Cj5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === UMID (CSV boxes) → YOLO (WBC-only) ===\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import re, shutil\n",
        "\n",
        "ROOT = Path(\"/content/drive/MyDrive/urine_uti/ml-services/kasundi\")\n",
        "RAW  = ROOT / \"data/raw/UMID\"\n",
        "OUT  = ROOT / \"data/interim/wbc_umid_yolo\"\n",
        "\n",
        "# ----- 1) Canonical class + normalizer -----\n",
        "CANONICAL = {\"WBC\": 0}\n",
        "WBC_ALIASES = {\n",
        "    \"wbc\",\"wbcs\",\"white blood cell\",\"white blood cells\",\n",
        "    \"leukocyte\",\"leukocytes\",\"leuco\",\"leukocytes\",\n",
        "    \"pus\",\"pus cell\",\"pus cells\",\"pus_cell\",\"pus_cells\"\n",
        "}\n",
        "def normalize_label(name: str):\n",
        "    if not name: return None\n",
        "    s = name.strip().lower()\n",
        "    s = s.replace(\"-\", \" \").replace(\"_\", \" \")\n",
        "    s = re.sub(r\"\\s+\",\" \", s)\n",
        "    return \"WBC\" if s in WBC_ALIASES else None\n",
        "\n",
        "# ----- 2) Make output dirs -----\n",
        "for split in [\"train\",\"val\",\"test\"]:\n",
        "    (OUT / f\"images/{split}\").mkdir(parents=True, exist_ok=True)\n",
        "    (OUT / f\"labels/{split}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ----- 3) Helper to resolve image path -----\n",
        "def resolve_img(src_str: str):\n",
        "    p = RAW / src_str\n",
        "    if p.exists(): return p\n",
        "    # try under images/\n",
        "    p = RAW / \"images\" / Path(src_str).name\n",
        "    if p.exists(): return p\n",
        "    # try jpg/png by stem\n",
        "    stem = Path(src_str).stem\n",
        "    for ext in (\".jpg\",\".png\",\".jpeg\",\".bmp\",\".tif\",\".tiff\"):\n",
        "        q = RAW / \"images\" / (stem+ext)\n",
        "        if q.exists(): return q\n",
        "        q2 = RAW / (stem+ext)\n",
        "        if q2.exists(): return q2\n",
        "    return None\n",
        "\n",
        "# ----- 4) Convert one split -----\n",
        "def convert_umid_split(csv_basename: str, split: str):\n",
        "    csv_path = RAW / f\"{csv_basename}.csv\"\n",
        "    if not csv_path.exists():\n",
        "        print(f\"[WARN] Missing {csv_path}\")\n",
        "        return 0,0\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "    lower = {c.lower(): c for c in df.columns}\n",
        "\n",
        "    # try to find columns in a tolerant way\n",
        "    fn_col  = lower.get(\"filename\") or lower.get(\"image\") or lower.get(\"path\") or list(df.columns)[0]\n",
        "    x1_col  = lower.get(\"xmin\") or lower.get(\"x1\")\n",
        "    y1_col  = lower.get(\"ymin\") or lower.get(\"y1\")\n",
        "    x2_col  = lower.get(\"xmax\") or lower.get(\"x2\")\n",
        "    y2_col  = lower.get(\"ymax\") or lower.get(\"y2\")\n",
        "    lbl_col = lower.get(\"class\") or lower.get(\"label\") or lower.get(\"name\") or lower.get(\"category\")\n",
        "\n",
        "    if not all([fn_col, x1_col, y1_col, x2_col, y2_col, lbl_col]):\n",
        "        raise ValueError(f\"Could not resolve columns in {csv_path}. Columns = {list(df.columns)}\")\n",
        "\n",
        "    wrote_imgs = wrote_boxes = 0\n",
        "\n",
        "    # group rows by image file\n",
        "    for img_key, rows in df.groupby(fn_col):\n",
        "        src = resolve_img(str(img_key))\n",
        "        if not src:\n",
        "            # print(f\"[MISS] {img_key}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            W, H = Image.open(src).size\n",
        "        except Exception as e:\n",
        "            # print(f\"[OPEN FAIL] {src}: {e}\")\n",
        "            continue\n",
        "\n",
        "        lines = []\n",
        "        for _, r in rows.iterrows():\n",
        "            cname = normalize_label(str(r[lbl_col]))\n",
        "            if cname != \"WBC\":\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                x1 = float(r[x1_col]); y1 = float(r[y1_col])\n",
        "                x2 = float(r[x2_col]); y2 = float(r[y2_col])\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # clamp and skip degenerate boxes\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(W, x2), min(H, y2)\n",
        "            if x2 <= x1 or y2 <= y1:\n",
        "                continue\n",
        "\n",
        "            cx, cy = (x1 + x2)/2.0, (y1 + y2)/2.0\n",
        "            w,  h  = (x2 - x1), (y2 - y1)\n",
        "            lines.append(f\"{CANONICAL['WBC']} {cx/W:.6f} {cy/H:.6f} {w/W:.6f} {h/H:.6f}\")\n",
        "\n",
        "        if lines:\n",
        "            dst_img = OUT / f\"images/{split}\" / src.name\n",
        "            dst_txt = OUT / f\"labels/{split}\" / (src.stem + \".txt\")\n",
        "            if not dst_img.exists():\n",
        "                shutil.copyfile(src, dst_img)\n",
        "            with open(dst_txt, \"w\") as f:\n",
        "                f.write(\"\\n\".join(lines))\n",
        "            wrote_imgs += 1\n",
        "            wrote_boxes += len(lines)\n",
        "\n",
        "    print(f\"[{split}] wrote {wrote_imgs} images, {wrote_boxes} boxes\")\n",
        "    return wrote_imgs, wrote_boxes\n",
        "\n",
        "# ----- 5) Run all splits + quick summary -----\n",
        "convert_umid_split(\"train\", \"train\")\n",
        "convert_umid_split(\"val\",   \"val\")\n",
        "convert_umid_split(\"test\",  \"test\")\n",
        "\n",
        "# summary\n",
        "import glob\n",
        "for split in [\"train\",\"val\",\"test\"]:\n",
        "    ni = len(glob.glob(str(OUT/f\"images/{split}/*.*\")))\n",
        "    nl = len(glob.glob(str(OUT/f\"labels/{split}/*.txt\")))\n",
        "    print(f\"SUMMARY {split}: {ni} images, {nl} label files → {OUT}/images/{split}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqZRtZYmCt33",
        "outputId": "22e84ac0-8ef4-4c3a-9fa7-f68213ccb095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] wrote 164 images, 703 boxes\n",
            "[val] wrote 24 images, 115 boxes\n",
            "[test] wrote 33 images, 168 boxes\n",
            "SUMMARY train: 164 images, 164 label files → /content/drive/MyDrive/urine_uti/ml-services/kasundi/data/interim/wbc_umid_yolo/images/train\n",
            "SUMMARY val: 24 images, 24 label files → /content/drive/MyDrive/urine_uti/ml-services/kasundi/data/interim/wbc_umid_yolo/images/val\n",
            "SUMMARY test: 33 images, 33 label files → /content/drive/MyDrive/urine_uti/ml-services/kasundi/data/interim/wbc_umid_yolo/images/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Urinary-Sediment (Pascal VOC XML) → YOLO (WBC only) with normalization\n",
        "#We parse each XML, keep only objects where <name> maps to \"WBC\" via our normalizer"
      ],
      "metadata": {
        "id": "QjnHKPxeC7R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Urinary-Sediment (VOC XML) → YOLO (WBC-only) ===\n",
        "from pathlib import Path\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "import shutil, re, glob\n",
        "\n",
        "# Set your project root\n",
        "ROOT = Path(\"/content/drive/MyDrive/urine_uti/ml-services/kasundi\")\n",
        "US_ROOT = ROOT / \"data/raw/Urinary_Sediment\"   # expects JPEGImages/, Annotations/, ImageSets/Main/\n",
        "US_OUT  = ROOT / \"data/interim/wbc_us_yolo\"\n",
        "\n",
        "# -------- WBC normalizer (includes 'leuko') ----------\n",
        "def is_wbc_label(name: str) -> bool:\n",
        "    if not name:\n",
        "        return False\n",
        "    s = name.lower()\n",
        "    s = s.replace(\"_\",\" \").replace(\"-\",\" \")\n",
        "    s = re.sub(r\"[^a-z ]\",\"\", s)\n",
        "    s = re.sub(r\"\\s+\",\" \", s).strip()\n",
        "    # include 'leuko' short form explicitly\n",
        "    keywords = [\n",
        "        \"wbc\", \"white blood cell\", \"white blood cells\",\n",
        "        \"leuko\", \"leukocyte\", \"leukocytes\", \"leucocyte\", \"leucocytes\",\n",
        "        \"pus cell\", \"pus cells\", \"pus\"\n",
        "    ]\n",
        "    return any(k in s for k in keywords)\n",
        "\n",
        "CLASS_ID_WBC = 0\n",
        "\n",
        "# make output dirs\n",
        "for split in [\"train\",\"val\",\"test\"]:\n",
        "    (US_OUT / f\"images/{split}\").mkdir(parents=True, exist_ok=True)\n",
        "    (US_OUT / f\"labels/{split}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def resolve_img_xml(img_id: str):\n",
        "    stem = Path(img_id).stem  # handle ids with or without .jpg\n",
        "    xml_path = US_ROOT / \"Annotations\" / f\"{stem}.xml\"\n",
        "    if not xml_path.exists():\n",
        "        hits = list((US_ROOT/\"Annotations\").glob(f\"{stem}*.xml\"))\n",
        "        xml_path = hits[0] if hits else None\n",
        "\n",
        "    # prefer .jpg then .png; else glob anything with same stem\n",
        "    img_path = US_ROOT / \"JPEGImages\" / f\"{stem}.jpg\"\n",
        "    if not img_path.exists():\n",
        "        alt = US_ROOT / \"JPEGImages\" / f\"{stem}.png\"\n",
        "        img_path = alt if alt.exists() else None\n",
        "    if img_path is None or not img_path.exists():\n",
        "        hits = list((US_ROOT/\"JPEGImages\").glob(f\"{stem}.*\"))\n",
        "        img_path = hits[0] if hits else None\n",
        "\n",
        "    return img_path, xml_path\n",
        "\n",
        "def convert_us_split(split: str):\n",
        "    ids_file = US_ROOT / \"ImageSets/Main\" / f\"{split}.txt\"\n",
        "    if not ids_file.exists():\n",
        "        print(f\"[WARN] {ids_file} not found. Skipping {split}.\")\n",
        "        return\n",
        "\n",
        "    ids = [l.strip() for l in open(ids_file) if l.strip()]\n",
        "    wrote_imgs = wrote_boxes = 0\n",
        "\n",
        "    for img_id in ids:\n",
        "        img_path, xml_path = resolve_img_xml(img_id)\n",
        "        if not (img_path and xml_path and img_path.exists() and xml_path.exists()):\n",
        "            # uncomment to debug missing pairs:\n",
        "            # print(f\"[MISS] {img_id} img:{img_path} xml:{xml_path}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            W, H = Image.open(img_path).size\n",
        "            root = ET.parse(xml_path).getroot()\n",
        "        except Exception as e:\n",
        "            # print(\"Parse/open fail:\", img_path, xml_path, e)\n",
        "            continue\n",
        "\n",
        "        lines = []\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name = (obj.findtext(\"name\") or \"\").strip()\n",
        "            if not is_wbc_label(name):   # <-- catches 'leuko'\n",
        "                continue\n",
        "            b = obj.find(\"bndbox\")\n",
        "            if b is None:\n",
        "                continue\n",
        "            x1 = float(b.findtext(\"xmin\")); y1 = float(b.findtext(\"ymin\"))\n",
        "            x2 = float(b.findtext(\"xmax\")); y2 = float(b.findtext(\"ymax\"))\n",
        "            # clamp\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(W, x2), min(H, y2)\n",
        "            if x2 <= x1 or y2 <= y1:\n",
        "                continue\n",
        "            cx, cy = (x1+x2)/2.0, (y1+y2)/2.0\n",
        "            w,  h  = (x2-x1), (y2-y1)\n",
        "            lines.append(f\"{CLASS_ID_WBC} {cx/W:.6f} {cy/H:.6f} {w/W:.6f} {h/H:.6f}\")\n",
        "\n",
        "        if lines:\n",
        "            dst_img = US_OUT / f\"images/{split}\" / img_path.name\n",
        "            dst_txt = US_OUT / f\"labels/{split}\" / (img_path.stem + \".txt\")\n",
        "            if not dst_img.exists():\n",
        "                shutil.copyfile(img_path, dst_img)\n",
        "            with open(dst_txt, \"w\") as f:\n",
        "                f.write(\"\\n\".join(lines))\n",
        "            wrote_imgs += 1\n",
        "            wrote_boxes += len(lines)\n",
        "\n",
        "    print(f\"[{split}] wrote {wrote_imgs} images, {wrote_boxes} boxes\")\n",
        "\n",
        "# run conversion\n",
        "convert_us_split(\"train\")\n",
        "convert_us_split(\"val\")\n",
        "convert_us_split(\"test\")\n",
        "\n",
        "# quick summary\n",
        "import glob\n",
        "for split in [\"train\",\"val\",\"test\"]:\n",
        "    ni = len(glob.glob(str(US_OUT / f\"images/{split}/*.*\")))\n",
        "    nl = len(glob.glob(str(US_OUT / f\"labels/{split}/*.txt\")))\n",
        "    print(f\"SUMMARY {split}: {ni} images, {nl} label files\")"
      ],
      "metadata": {
        "id": "pyc6pW8lC-qC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4144b730-a238-411c-94b8-30472a50553a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] wrote 1052 images, 5112 boxes\n",
            "[val] wrote 232 images, 796 boxes\n",
            "[test] wrote 54 images, 261 boxes\n",
            "SUMMARY train: 1052 images, 1052 label files\n",
            "SUMMARY val: 232 images, 232 label files\n",
            "SUMMARY test: 54 images, 54 label files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge the two interim sets into one training set.here, concatenate by split."
      ],
      "metadata": {
        "id": "E6LYdy5SDBl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "FINAL = ROOT / \"data/processed/wbc_detect\"\n",
        "for d in [\"images/train\",\"images/val\",\"images/test\",\"labels/train\",\"labels/val\",\"labels/test\"]:\n",
        "    (FINAL / d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def copy_all(src_dir, dst_dir):\n",
        "    for p in glob.glob(str(src_dir)):\n",
        "        shutil.copy(p, dst_dir)\n",
        "\n",
        "# UMID\n",
        "copy_all(OUT / \"images/train/*\", FINAL / \"images/train\")\n",
        "copy_all(OUT / \"labels/train/*\", FINAL / \"labels/train\")\n",
        "copy_all(OUT / \"images/val/*\",   FINAL / \"images/val\")\n",
        "copy_all(OUT / \"labels/val/*\",   FINAL / \"labels/val\")\n",
        "copy_all(OUT / \"images/test/*\",  FINAL / \"images/test\")\n",
        "copy_all(OUT / \"labels/test/*\",  FINAL / \"labels/test\")\n",
        "\n",
        "# Urinary-Sediment\n",
        "copy_all(US_OUT / \"images/train/*\", FINAL / \"images/train\")\n",
        "copy_all(US_OUT / \"labels/train/*\", FINAL / \"labels/train\")\n",
        "copy_all(US_OUT / \"images/val/*\",   FINAL / \"images/val\")\n",
        "copy_all(US_OUT / \"labels/val/*\",   FINAL / \"labels/val\")\n",
        "copy_all(US_OUT / \"images/test/*\",  FINAL / \"images/test\")\n",
        "copy_all(US_OUT / \"labels/test/*\",  FINAL / \"labels/test\")"
      ],
      "metadata": {
        "id": "HPmps2z5DL2O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}